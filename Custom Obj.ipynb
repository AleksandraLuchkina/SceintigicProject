{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('facies_vectors_0.csv')\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "data = data.fillna(data['PE'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running example to used customized objective function\n",
      "SHRIMPLIN\n",
      "471   471\n",
      "well, boosting of trees,  0.021656050955414015\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print('start running example to used customized objective function')\n",
    "\n",
    "params = {'max_depth': 2, 'eta': 0.1, 'silent': 1,\n",
    "          'objective': 'multi:softprob', 'num_class': 9}\n",
    "\n",
    "num_round = 2\n",
    "def my_softmax(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    labels = OneHotEncoder(sparse=False, n_values=9).fit_transform(labels.reshape(-1, 1))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0-preds)\n",
    "#     hess = -preds\n",
    "    K1 = [[0]*9]*dtrain.num_row()\n",
    "    dtrain_len = dtrain.num_row()\n",
    "    for object_ in range(1, dtrain_len-1):\n",
    "        if(max(preds[object_-1]) == max(preds[object_+1])):\n",
    "            for class_  in range(9):\n",
    "                if (preds[object_+1][class_] - preds[object_-1][class_] != 0):\n",
    "                    K1[object_][class_] = (preds[object_+1][class_] - preds[object_-1][class_])/abs(preds[object_+1][class_] - preds[object_-1][class_])\n",
    "    K2 = [[0]*9]*dtrain.num_row()\n",
    "    for object_ in range(1, dtrain_len-1):\n",
    "        if(max(preds[object_-1]) == max(preds[object_+1])):\n",
    "            for class_  in range(9):\n",
    "                if (preds[object_+1][class_] - preds[object_][class_] != 0):\n",
    "                    K2[object_][class_] = (preds[object_+1][class_] - preds[object_][class_])/abs(preds[object_+1][class_] - preds[object_][class_])\n",
    "    K3 = [[0]*9]*dtrain.num_row()\n",
    "    for object_ in range(1, dtrain_len-1):\n",
    "        if(max(preds[object_-1]) == max(preds[object_+1])):\n",
    "            for class_  in range(9):\n",
    "                if (preds[object_][class_] - preds[object_-1][class_] != 0):\n",
    "                    K3[object_][class_] = (preds[object_][class_] - preds[object_-1][class_])/abs(preds[object_][class_] - preds[object_-1][class_])\n",
    "    \n",
    "    L = [[0]*9]*dtrain.num_row()\n",
    "    \n",
    "    for object_ in range(1, dtrain_len-1):\n",
    "        if(max(preds[object_-1]) == max(preds[object_+1])):\n",
    "            for index in range(9):\n",
    "                for class_ in range(9):\n",
    "                    L[object_][index] = L[object_][index] - 2 * K1[object_][class_] * (preds[object_-1][class_] * preds[object_-1][index] - preds[object_+1][class_] * preds[object_+1][index])\n",
    "                    L[object_][index] = L[object_][index] + K2[object_][class_] * (preds[object_][class_] * preds[object_][index] - preds[object_+1][class_] * preds[object_+1][index])\n",
    "                    L[object_][index] = L[object_][index] + K3[object_][class_] * (preds[object_-1][class_] * preds[object_-1][index] - preds[object_][class_] * preds[object_][index])\n",
    "                L[object_][index] = L[object_][index] - 2 * K1[object_][index] * (preds[object_+1][index] - preds[object_-1][index])\n",
    "                L[object_][index] = L[object_][index] + K2[object_][index] * (preds[object_+1][index] - preds[object_][index])\n",
    "                L[object_][index] = L[object_][index] + K3[object_][index] * (preds[object_][index] - preds[object_-1][index])\n",
    "    grad = grad +  np.array(L)\n",
    "    return grad.flatten(), hess.flatten()\n",
    "\n",
    "def softmaxobj(preds, dtrain):\n",
    "    \"\"\"Softmax objective.\n",
    "    Args:\n",
    "        preds: (N, K) array, N = #data, K = #classes. \n",
    "        dtrain: DMatrix object with training data.\n",
    "    \n",
    "    Returns:\n",
    "        grad: N*K array with gradient values.\n",
    "        hess: N*K array with second-order gradient values.\n",
    "    \"\"\"\n",
    "    # Label is a vector of class indices for each input example\n",
    "    labels = dtrain.get_label()\n",
    "    # When objective=softprob, preds has shape (N, K)\n",
    "    labels = OneHotEncoder(sparse=False, n_values=9).fit_transform(labels.reshape(-1, 1))\n",
    "    grad = preds - labels\n",
    "    hess = 2.0 * preds * (1.0-preds)\n",
    "    # Return as 1-d vectors\n",
    "    return grad.flatten(), hess.flatten()\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)\n",
    "\n",
    "def my_evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    evals = 0\n",
    "    for index in range(len(labels)):\n",
    "        if (labels[index] == preds[index]):\n",
    "            evals += 1\n",
    "    print(\"Eval error\", evals/len(labels))\n",
    "    return 'error', evals / len(labels)\n",
    "\n",
    "acc = 0\n",
    "# for well in set(data['Well Name']):\n",
    "well = 'SHRIMPLIN'\n",
    "print(well)\n",
    "train = data[data['Well Name'] != well]\n",
    "test = data[data['Well Name'] == well]\n",
    "X_train = train[feature_names].values\n",
    "y_train = train['Facies'].values\n",
    "X_test = test[feature_names].values\n",
    "y_test = test['Facies'].values\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "dtrain = xgb.DMatrix(X_train_norm, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_norm, label=y_test)\n",
    "watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "#     model = xgb.train(params, dtrain, 100)\n",
    "#     bst = xgb.train(param, dtrain, num_round, watchlist, obj=my_logregobj)\n",
    "model = xgb.Booster(params, [dtrain])\n",
    "for _ in range(100):\n",
    "    pred = model.predict(dtrain)\n",
    "    g, h = my_softmax(pred, dtrain)\n",
    "    model.boost(dtrain, g, h)\n",
    "# Evalute\n",
    "yhat = model.predict(dtest)\n",
    "yhat_labels = np.argmax(yhat, axis=1)\n",
    "#     ypred = bst.predict(dtest)\n",
    "print(len(y_test), \" \", len(yhat_labels))\n",
    "acc += f1_score(y_test, yhat_labels, average='micro')\n",
    "print('well, boosting of trees, ', acc/10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running example to used customized objective function\n",
      "KIMZEY A\n",
      "439   439\n",
      "ALEXANDER D\n",
      "466   466\n",
      "SHRIMPLIN\n",
      "471   471\n",
      "Recruit F9\n",
      "80   80\n",
      "CROSS H CATTLE\n",
      "501   501\n",
      "SHANKLE\n",
      "449   449\n",
      "CHURCHMAN BIBLE\n",
      "404   404\n",
      "NEWBY\n",
      "463   463\n",
      "LUKE G U\n",
      "461   461\n",
      "NOLAN\n",
      "415   415\n",
      "well, boosting of trees,  0.509848495390181\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import math\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print('start running example to used customized objective function')\n",
    "\n",
    "params = {'max_depth': 2, 'eta': 0.1, 'silent': 1,\n",
    "          'objective': 'multi:softprob', 'num_class': 9}\n",
    "\n",
    "num_round = 2\n",
    "def my_softmax(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    labels = OneHotEncoder(sparse=False, n_values=9).fit_transform(labels.reshape(-1, 1))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1.0-preds)\n",
    "    dtrain_len = dtrain.num_row()\n",
    "    K3 = [[0]*9]*dtrain.num_row()\n",
    "    for object_ in range(1, dtrain_len-1):\n",
    "        for class_  in range(9):\n",
    "            if (preds[object_][class_] - preds[object_-1][class_] != 0):\n",
    "                K3[object_][class_] = (preds[object_][class_] - preds[object_-1][class_])/abs(preds[object_][class_] - preds[object_-1][class_])\n",
    "    \n",
    "    L = [[0]*9]*dtrain.num_row()\n",
    "    \n",
    "    for object_ in range(1, dtrain_len-1):\n",
    "        ind1 = np.where(preds[object_-1] == max(preds[object_-1]))[0][0]\n",
    "        ind2 = np.where(preds[object_+1] == max(preds[object_+1]))[0][0]\n",
    "        if(ind1 == ind2):\n",
    "            for index in range(9):\n",
    "                for class_ in range(9):\n",
    "                    L[object_][index] = L[object_][index] + K3[object_][class_] * (preds[object_-1][class_] * preds[object_-1][index] - preds[object_][class_] * preds[object_][index])\n",
    "                L[object_][index] = L[object_][index] + K3[object_][index] * (preds[object_][index] - preds[object_-1][index])\n",
    "    grad = grad + np.array(L)\n",
    "    \n",
    "#     H = [[0]*9]*dtrain.num_row()\n",
    "#     for object_ in range(1, dtrain_len-1):\n",
    "#         if(max(preds[object_-1]) == max(preds[object_+1])):\n",
    "#             for index in range(9):\n",
    "#                 for class_ in range(9):\n",
    "#                     H[object_][index] = H[object_][index] + (preds[object_][index]*preds[object_][class_] - preds[object_-1][index]*preds[object_-1][class_] + 2*preds[object_-1][class_]*preds[object_-1][index]*preds[object_-1][index] - 2*preds[object_-1][class_]*preds[object_][index]*preds[object_][index])*abs(preds[object_-1][index] - preds[object_][index])/(preds[object_-1][index] - preds[object_][index])\n",
    "#                 H[object_][index] = H[object_][index] + (preds[object_][index] * preds[object_][index] - preds[object_-1][index] * preds[object_-1][index])*abs(preds[object_-1][index] - preds[object_][index])/(preds[object_-1][index] - preds[object_][index])\n",
    "#                 H[object_][index] = H[object_][index] + (2*(preds[object_-1][index] - preds[object_][index] + preds[object_][index]* preds[object_][index] - preds[object_-1][index] * preds[object_-1][index]) - 1)*abs(preds[object_-1][index] - preds[object_][index])/(preds[object_-1][index] - preds[object_][index])\n",
    "#     hess = hess + np.array(H)\n",
    "    return grad.flatten(), hess.flatten()\n",
    "\n",
    "def softmaxobj(preds, dtrain):\n",
    "    \"\"\"Softmax objective.\n",
    "    Args:\n",
    "        preds: (N, K) array, N = #data, K = #classes. \n",
    "        dtrain: DMatrix object with training data.\n",
    "    \n",
    "    Returns:\n",
    "        grad: N*K array with gradient values.\n",
    "        hess: N*K array with second-order gradient values.\n",
    "    \"\"\"\n",
    "    # Label is a vector of class indices for each input example\n",
    "    labels = dtrain.get_label()\n",
    "    # When objective=softprob, preds has shape (N, K)\n",
    "    labels = OneHotEncoder(sparse=False, n_values=9).fit_transform(labels.reshape(-1, 1))\n",
    "    grad = preds - labels\n",
    "    hess = 2.0 * preds * (1.0-preds)\n",
    "    # Return as 1-d vectors\n",
    "    return grad.flatten(), hess.flatten()\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'error', float(sum(labels != (preds > 0.0))) / len(labels)\n",
    "\n",
    "def my_evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    evals = 0\n",
    "    for index in range(len(labels)):\n",
    "        if (labels[index] == preds[index]):\n",
    "            evals += 1\n",
    "    print(\"Eval error\", evals/len(labels))\n",
    "    return 'error', evals / len(labels)\n",
    "\n",
    "acc = 0\n",
    "# for well in set(data['Well Name']):\n",
    "for well in set(data['Well Name']):\n",
    "# well = 'SHRIMPLIN'\n",
    "    print(well)\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    dtrain = xgb.DMatrix(X_train_norm, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test_norm, label=y_test)\n",
    "    watchlist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "    #     model = xgb.train(params, dtrain, 100)\n",
    "    #     bst = xgb.train(param, dtrain, num_round, watchlist, obj=my_logregobj)\n",
    "    model = xgb.Booster(params, [dtrain])\n",
    "    for _ in range(100):\n",
    "        pred = model.predict(dtrain)\n",
    "        g, h = my_softmax(pred, dtrain)\n",
    "        model.boost(dtrain, g, h)\n",
    "    # Evalute\n",
    "    yhat = model.predict(dtest)\n",
    "    yhat_labels = np.argmax(yhat, axis=1)\n",
    "    #     ypred = bst.predict(dtest)\n",
    "    print(len(y_test), \" \", len(yhat_labels))\n",
    "    acc += f1_score(y_test, yhat_labels, average='micro')\n",
    "print('well, boosting of trees, ', acc/10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584703947368421\n"
     ]
    }
   ],
   "source": [
    "yhat1 = model.predict(dtrain)\n",
    "yhat1_labels = np.argmax(yhat1, axis=1)\n",
    "print(f1_score(y_train, yhat1_labels, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
