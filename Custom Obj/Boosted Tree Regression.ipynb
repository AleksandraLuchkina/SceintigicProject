{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "try:\n",
    "    # For python2\n",
    "    from itertools import izip as zip\n",
    "    LARGE_NUMBER = sys.maxint\n",
    "except ImportError:\n",
    "    # For python3\n",
    "    LARGE_NUMBER = sys.maxsize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "\n",
    "class TreeNode(object):\n",
    "    def __init__(self):\n",
    "        self.is_leaf = False\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.split_feature_id = None\n",
    "        self.split_val = None\n",
    "        self.weight = None\n",
    "\n",
    "    def _calc_split_gain(self, G, H, G_l, H_l, G_r, H_r, lambd):\n",
    "        \"\"\"\n",
    "        Loss reduction\n",
    "        (Refer to Eq7 of Reference[1])\n",
    "        \"\"\"\n",
    "        def calc_term(g, h):\n",
    "            return np.square(g) / (h + lambd)\n",
    "        return calc_term(G_l, H_l) + calc_term(G_r, H_r) - calc_term(G, H)\n",
    "\n",
    "    def _calc_leaf_weight(self, grad, hessian, lambd):\n",
    "        \"\"\"\n",
    "        Calculate the optimal weight of this leaf node.\n",
    "        (Refer to Eq5 of Reference[1])\n",
    "        \"\"\"\n",
    "        return np.sum(grad) / (np.sum(hessian) + lambd)\n",
    "\n",
    "    def build(self, instances, grad, hessian, shrinkage_rate, depth, param):\n",
    "        \"\"\"\n",
    "        Exact Greedy Alogirithm for Split Finidng\n",
    "        (Refer to Algorithm1 of Reference[1])\n",
    "        \"\"\"\n",
    "        assert instances.shape[0] == len(grad) == len(hessian)\n",
    "        if depth > param['max_depth']:\n",
    "            self.is_leaf = True\n",
    "            self.weight = self._calc_leaf_weight(grad, hessian, param['lambda']) * shrinkage_rate\n",
    "            return\n",
    "        G = np.sum(grad)\n",
    "        H = np.sum(hessian)\n",
    "        best_gain = 0.\n",
    "        best_feature_id = None\n",
    "        best_val = 0.\n",
    "        best_left_instance_ids = None\n",
    "        best_right_instance_ids = None\n",
    "        for feature_id in range(instances.shape[1]):\n",
    "            G_l, H_l = 0., 0.\n",
    "            sorted_instance_ids = instances[:,feature_id].argsort()\n",
    "            for j in range(sorted_instance_ids.shape[0]):\n",
    "                G_l += grad[sorted_instance_ids[j]]\n",
    "                H_l += hessian[sorted_instance_ids[j]]\n",
    "                G_r = G - G_l\n",
    "                H_r = H - H_l\n",
    "                current_gain = self._calc_split_gain(G, H, G_l, H_l, G_r, H_r, param['lambda'])\n",
    "                if current_gain > best_gain:\n",
    "                    best_gain = current_gain\n",
    "                    best_feature_id = feature_id\n",
    "                    best_val = instances[sorted_instance_ids[j]][feature_id]\n",
    "                    best_left_instance_ids = sorted_instance_ids[:j+1]\n",
    "                    best_right_instance_ids = sorted_instance_ids[j+1:]\n",
    "        if best_gain < param['min_split_gain']:\n",
    "            self.is_leaf = True\n",
    "            self.weight = self._calc_leaf_weight(grad, hessian, param['lambda']) * shrinkage_rate\n",
    "        else:\n",
    "            self.split_feature_id = best_feature_id\n",
    "            self.split_val = best_val\n",
    "\n",
    "            self.left_child = TreeNode()\n",
    "            self.left_child.build(instances[best_left_instance_ids],\n",
    "                                  grad[best_left_instance_ids],\n",
    "                                  hessian[best_left_instance_ids],\n",
    "                                  shrinkage_rate,\n",
    "                                  depth+1, param)\n",
    "\n",
    "            self.right_child = TreeNode()\n",
    "            self.right_child.build(instances[best_right_instance_ids],\n",
    "                                   grad[best_right_instance_ids],\n",
    "                                   hessian[best_right_instance_ids],\n",
    "                                   shrinkage_rate,\n",
    "                                   depth+1, param)\n",
    "\n",
    "    def predict(self, x):\n",
    "        if self.is_leaf:\n",
    "            return self.weight\n",
    "        else:\n",
    "            if x[self.split_feature_id] <= self.split_val:\n",
    "                return self.left_child.predict(x)\n",
    "            else:\n",
    "                return self.right_child.predict(x)\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    ''' Classification and regression tree for tree ensemble '''\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "\n",
    "    def build(self, instances, grad, hessian, shrinkage_rate, param):\n",
    "        assert len(instances) == len(grad) == len(hessian)\n",
    "        self.root = TreeNode()\n",
    "        current_depth = 0\n",
    "        self.root.build(instances, grad, hessian, shrinkage_rate, current_depth, param)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.root.predict(x)\n",
    "\n",
    "\n",
    "class GBT(object):\n",
    "    def __init__(self):\n",
    "        self.params = {'gamma': 0.,\n",
    "                       'lambda': 1.,\n",
    "                       'min_split_gain': 0.1,\n",
    "                       'max_depth': 5,\n",
    "                       'learning_rate': 0.3,\n",
    "                       }\n",
    "        self.best_iteration = None\n",
    "\n",
    "    def _calc_training_data_scores(self, train_set, models):\n",
    "        if len(models) == 0:\n",
    "            return None\n",
    "        X = train_set.X\n",
    "        scores = np.zeros(len(X))\n",
    "        for i in range(len(X)):\n",
    "            scores[i] = self.predict(X[i], models=models)\n",
    "        return scores\n",
    "    \n",
    "\n",
    "    def _calc_l2_gradient(self, train_set, scores):\n",
    "        labels = train_set.y\n",
    "        hessian = np.full(len(labels), 2)\n",
    "        if scores is None:\n",
    "            grad = np.random.uniform(size=len(labels))\n",
    "        else:\n",
    "            grad = np.array([2 * (labels[i] - scores[i]) for i in range(len(labels))])\n",
    "        return grad, hessian\n",
    "\n",
    "    def _calc_gradient(self, train_set, scores):\n",
    "        \"\"\"For now, only L2 loss is supported\"\"\"\n",
    "        return self._calc_l2_gradient(train_set, scores)\n",
    "\n",
    "    def _calc_l2_loss(self, models, data_set):\n",
    "        errors = []\n",
    "        for x, y in zip(data_set.X, data_set.y):\n",
    "            errors.append(y - self.predict(x, models))\n",
    "        return np.mean(np.square(errors))\n",
    "\n",
    "    def _calc_loss(self, models, data_set):\n",
    "        \"\"\"For now, only L2 loss is supported\"\"\"\n",
    "        return self._calc_l2_loss(models, data_set)\n",
    "\n",
    "    def _build_learner(self, train_set, grad, hessian, shrinkage_rate):\n",
    "        learner = Tree()\n",
    "        learner.build(train_set.X, grad, hessian, shrinkage_rate, self.params)\n",
    "        return learner\n",
    "\n",
    "    def train(self, params, train_set, num_boost_round=20, valid_set=None, early_stopping_rounds=5):\n",
    "        self.params.update(params)\n",
    "        models = []\n",
    "        shrinkage_rate = 1.\n",
    "        best_iteration = None\n",
    "        best_val_loss = LARGE_NUMBER\n",
    "        train_start_time = time.time()\n",
    "\n",
    "        print(\"Training until validation scores don't improve for {} rounds.\"\n",
    "              .format(early_stopping_rounds))\n",
    "        for iter_cnt in range(num_boost_round):\n",
    "            iter_start_time = time.time()\n",
    "            scores = self._calc_training_data_scores(train_set, models)\n",
    "            grad, hessian = self._calc_gradient(train_set, scores)\n",
    "            learner = self._build_learner(train_set, grad, hessian, shrinkage_rate)\n",
    "            if iter_cnt > 0:\n",
    "                shrinkage_rate *= self.params['learning_rate']\n",
    "            models.append(learner)\n",
    "            train_loss = self._calc_loss(models, train_set)\n",
    "            val_loss = self._calc_loss(models, valid_set) if valid_set else None\n",
    "            val_loss_str = '{:.10f}'.format(val_loss) if val_loss else '-'\n",
    "            print(\"Iter {:>3}, Train's L2: {:.10f}, Valid's L2: {}, Elapsed: {:.2f} secs\"\n",
    "                  .format(iter_cnt, train_loss, val_loss_str, time.time() - iter_start_time))\n",
    "            if val_loss is not None and val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_iteration = iter_cnt\n",
    "            if iter_cnt - best_iteration >= early_stopping_rounds:\n",
    "                print(\"Early stopping, best iteration is:\")\n",
    "                print(\"Iter {:>3}, Train's L2: {:.10f}\".format(best_iteration, best_val_loss))\n",
    "                break\n",
    "\n",
    "        self.models = models\n",
    "        self.best_iteration = best_iteration\n",
    "        print(\"Training finished. Elapsed: {:.2f} secs\".format(time.time() - train_start_time))\n",
    "\n",
    "    def predict(self, x, models=None, num_iteration=None):\n",
    "        if models is None:\n",
    "            models = self.models\n",
    "        assert models is not None\n",
    "        return np.sum(m.predict(x) for m in models[:num_iteration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Start training...\n",
      "Training until validation scores don't improve for 5 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:209: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter   0, Train's L2: 23.9603734376, Valid's L2: 25.9775635996, Elapsed: 5.45 secs\n",
      "Iter   1, Train's L2: 8.5676006769, Valid's L2: 6.8969110854, Elapsed: 5.61 secs\n",
      "Iter   2, Train's L2: 7.6388028112, Valid's L2: 6.0243508594, Elapsed: 5.62 secs\n",
      "Iter   3, Train's L2: 7.4007471123, Valid's L2: 5.8150365710, Elapsed: 5.70 secs\n",
      "Iter   4, Train's L2: 7.3650547585, Valid's L2: 5.7738123750, Elapsed: 5.79 secs\n",
      "Iter   5, Train's L2: 7.3455167012, Valid's L2: 5.7574070372, Elapsed: 5.78 secs\n",
      "Iter   6, Train's L2: 7.3396126029, Valid's L2: 5.7524896939, Elapsed: 5.96 secs\n",
      "Iter   7, Train's L2: 7.3386949618, Valid's L2: 5.7514166800, Elapsed: 5.94 secs\n",
      "Iter   8, Train's L2: 7.3381652186, Valid's L2: 5.7509756305, Elapsed: 5.94 secs\n",
      "Iter   9, Train's L2: 7.3380063321, Valid's L2: 5.7508433508, Elapsed: 5.95 secs\n",
      "Iter  10, Train's L2: 7.3379613051, Valid's L2: 5.7508036701, Elapsed: 6.02 secs\n",
      "Iter  11, Train's L2: 7.3379470066, Valid's L2: 5.7507917662, Elapsed: 6.05 secs\n",
      "Iter  12, Train's L2: 7.3379427170, Valid's L2: 5.7507881950, Elapsed: 6.10 secs\n",
      "Iter  13, Train's L2: 7.3379414302, Valid's L2: 5.7507871237, Elapsed: 6.14 secs\n",
      "Iter  14, Train's L2: 7.3379410441, Valid's L2: 5.7507868023, Elapsed: 6.19 secs\n",
      "Iter  15, Train's L2: 7.3379409283, Valid's L2: 5.7507867059, Elapsed: 6.21 secs\n",
      "Iter  16, Train's L2: 7.3379408936, Valid's L2: 5.7507866769, Elapsed: 6.26 secs\n",
      "Iter  17, Train's L2: 7.3379408837, Valid's L2: 5.7507866683, Elapsed: 6.31 secs\n",
      "Iter  18, Train's L2: 7.3379408806, Valid's L2: 5.7507866657, Elapsed: 6.35 secs\n",
      "Iter  19, Train's L2: 7.3379408796, Valid's L2: 5.7507866649, Elapsed: 6.38 secs\n",
      "Training finished. Elapsed: 119.77 secs\n",
      "Start predicting...\n",
      "Accuracy  0.0\n",
      "The rmse of prediction is: 2.3980797871740887\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from tinygbt import Dataset, GBT\n",
    "\n",
    "\n",
    "print('Load data...')\n",
    "data = pd.read_csv('facies_vectors.csv')\n",
    "data = data.fillna(data['PE'].mean())\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "well = 'KIMZEY A'\n",
    "test = data[data['Well Name'] == well]\n",
    "train = data[data['Well Name'] != well]\n",
    "X_train = train[feature_names].values \n",
    "y_train = train['Facies'].values \n",
    "X_test = test[feature_names].values \n",
    "y_test = test['Facies'].values \n",
    "\n",
    "train_data = Dataset(X_train, y_train)\n",
    "eval_data = Dataset(X_test, y_test)\n",
    "\n",
    "params = {}\n",
    "\n",
    "print('Start training...')\n",
    "gbt = GBT()\n",
    "gbt.train(params,\n",
    "          train_data,\n",
    "          num_boost_round=20,\n",
    "          valid_set=eval_data,\n",
    "          early_stopping_rounds=5)\n",
    "\n",
    "print('Start predicting...')\n",
    "y_pred = []\n",
    "for x in X_test:\n",
    "    y_pred.append(gbt.predict(x, num_iteration=gbt.best_iteration))\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    acc = 0\n",
    "    for index in range (len(y)):\n",
    "        if (y[index] == y_pred[index]):\n",
    "            acc += 1\n",
    "    return acc/len(y)\n",
    "\n",
    "print(\"Accuracy \", accuracy(y_pred, y_test))\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.4290819719912244,\n",
       " 2.443196377982123,\n",
       " 2.441830485363464,\n",
       " 2.4374174870894976,\n",
       " 2.435553421660977,\n",
       " 1.9966533245892308,\n",
       " 1.9966533245892308,\n",
       " 1.5488140556307237,\n",
       " 1.5488140556307237,\n",
       " 1.5488140556307237,\n",
       " 4.101472283992752,\n",
       " 2.435553421660977,\n",
       " 1.6317820460736892,\n",
       " 2.724832465994971,\n",
       " 2.7588532960675027,\n",
       " 2.724832465994971,\n",
       " 2.4609091429991308,\n",
       " 2.4771014988950766,\n",
       " 2.0382014018233314,\n",
       " 2.0382014018233314,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.1269206257644826,\n",
       " 2.0023165063222788,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.5658207228362278,\n",
       " 2.442074838882777,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.1914874528800894,\n",
       " 2.1914874528800894,\n",
       " 2.2127125233682134,\n",
       " 2.217052735612049,\n",
       " 2.463299909370901,\n",
       " 1.652001286622045,\n",
       " 1.801884710101609,\n",
       " 3.0465589739992613,\n",
       " 4.353996521454094,\n",
       " 2.7823721615110815,\n",
       " 2.6092588016266554,\n",
       " 2.8948035373091083,\n",
       " 2.8948035373091083,\n",
       " 2.8948035373091083,\n",
       " 2.8948035373091083,\n",
       " 2.8948035373091083,\n",
       " 2.8948035373091083,\n",
       " 2.8948035373091083,\n",
       " 2.7996289351093937,\n",
       " 2.7996289351093937,\n",
       " 2.734048201988324,\n",
       " 2.734048201988324,\n",
       " 2.7573643013541482,\n",
       " 2.3345529929459112,\n",
       " 2.3345529929459112,\n",
       " 2.6236413248776778,\n",
       " 2.6236413248776778,\n",
       " 2.6236413248776778,\n",
       " 2.834916914407542,\n",
       " 2.834916914407542,\n",
       " 2.834916914407542,\n",
       " 2.766646654875895,\n",
       " 2.6844701080973112,\n",
       " 2.792569685321797,\n",
       " 2.6844701080973112,\n",
       " 2.7094779682542445,\n",
       " 2.8321102727342287,\n",
       " 2.6830751159541055,\n",
       " 2.9119726946302906,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.207975605384188,\n",
       " 2.869624713715272,\n",
       " 2.869624713715272,\n",
       " 2.9305251765286604,\n",
       " 2.9305251765286604,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 4.5702681937686975,\n",
       " 3.1684298032117564,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.465449234365178,\n",
       " 2.869708060875045,\n",
       " 2.888260542773415,\n",
       " 2.715147182888989,\n",
       " 2.869708060875045,\n",
       " 2.446896752466808,\n",
       " 2.465449234365178,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.66927352642993,\n",
       " 2.66927352642993,\n",
       " 2.66927352642993,\n",
       " 2.8419506507483274,\n",
       " 2.6555348230328417,\n",
       " 2.7395031545997988,\n",
       " 2.9138921995430267,\n",
       " 2.9138921995430267,\n",
       " 2.9138921995430267,\n",
       " 2.9138921995430267,\n",
       " 5.993863189552465,\n",
       " 2.443196377982123,\n",
       " 2.443196377982123,\n",
       " 2.6665757017805642,\n",
       " 2.6665757017805642,\n",
       " 2.6665757017805642,\n",
       " 2.6510545303173747,\n",
       " 2.471438317162029,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.0382014018233314,\n",
       " 4.143020361226854,\n",
       " 2.5497319341727196,\n",
       " 2.25201445484609,\n",
       " 2.425127814730516,\n",
       " 2.401811715364692,\n",
       " 2.401811715364692,\n",
       " 2.5264158348068952,\n",
       " 2.5497319341727196,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.5497319341727196,\n",
       " 2.5497319341727196,\n",
       " 3.1329909988010805,\n",
       " 5.095715193023068,\n",
       " 5.137979826778313,\n",
       " 5.137979826778313,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.1908897952227795,\n",
       " 5.208058952543961,\n",
       " 5.208058952543961,\n",
       " 5.208058952543961,\n",
       " 5.250323586299206,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.208058952543961,\n",
       " 2.869708060875045,\n",
       " 2.869708060875045,\n",
       " 2.9119726946302906,\n",
       " 2.4891613862220536,\n",
       " 2.94589478130978,\n",
       " 2.9138921995430267,\n",
       " 2.9259189823152845,\n",
       " 2.883654348560039,\n",
       " 2.799686016993082,\n",
       " 2.443196377982123,\n",
       " 2.4859440326647118,\n",
       " 1.6746454099158574,\n",
       " 2.6665757017805642,\n",
       " 2.652027063148435,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.442074838882777,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.5497319341727196,\n",
       " 2.5497319341727196,\n",
       " 2.8948035373091083,\n",
       " 2.7996289351093937,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 2.9119726946302906,\n",
       " 2.9119726946302906,\n",
       " 5.268876068197578,\n",
       " 5.268876068197578,\n",
       " 2.8649444434075906,\n",
       " 2.8900349547583932,\n",
       " 2.9167864731315722,\n",
       " 2.8982339912332025,\n",
       " 2.8982339912332025,\n",
       " 2.4939751647233352,\n",
       " 2.5077138681204234,\n",
       " 2.6270088926746844,\n",
       " 2.2241697018936377,\n",
       " 2.4563362008661467,\n",
       " 2.2410627515245185,\n",
       " 2.6665757017805642,\n",
       " 2.471788753820181,\n",
       " 2.471438317162029,\n",
       " 2.0382014018233314,\n",
       " 2.013193541666398,\n",
       " 2.013193541666398,\n",
       " 2.013193541666398,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.468831320173843,\n",
       " 4.134750182505621,\n",
       " 6.769440623682531,\n",
       " 6.46108642014778,\n",
       " 5.162987686935247,\n",
       " 5.120723053180002,\n",
       " 5.120723053180002,\n",
       " 5.1378922105011835,\n",
       " 2.0043314847469156,\n",
       " 2.0043314847469156,\n",
       " 5.129622031779951,\n",
       " 5.226611434442333,\n",
       " 5.129622031779951,\n",
       " 5.129622031779951,\n",
       " 2.888260542773415,\n",
       " 2.888260542773415,\n",
       " 2.888260542773415,\n",
       " 2.2241697018936377,\n",
       " 2.2241697018936377,\n",
       " 2.0075303599333507,\n",
       " 2.0075303599333507,\n",
       " 2.032538220090284,\n",
       " 2.0382014018233314,\n",
       " 2.0382014018233314,\n",
       " 2.0382014018233314,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.1269206257644826,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.442074838882777,\n",
       " 1.6307762161339214,\n",
       " 4.231739585168003,\n",
       " 3.1329909988010805,\n",
       " 5.120723053180002,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.22652808728256,\n",
       " 5.207975605384188,\n",
       " 2.9305251765286604,\n",
       " 2.888260542773415,\n",
       " 2.888260542773415,\n",
       " 2.465449234365178,\n",
       " 2.677193211178597,\n",
       " 2.677193211178597,\n",
       " 2.471438317162029,\n",
       " 2.7531901143344553,\n",
       " 2.7588532960675027,\n",
       " 2.723826636055203,\n",
       " 2.7505831173462694,\n",
       " 2.7505831173462694,\n",
       " 6.18618155905417,\n",
       " 6.418821786392534,\n",
       " 5.120723053180002,\n",
       " 5.162987686935247,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.162987686935247,\n",
       " 5.1908897952227795,\n",
       " 5.120723053180002,\n",
       " 5.120723053180002,\n",
       " 5.120723053180002,\n",
       " 5.120723053180002,\n",
       " 6.418821786392534,\n",
       " 6.418821786392534,\n",
       " 2.0043314847469156,\n",
       " 3.1257209004550384,\n",
       " 3.1257209004550384,\n",
       " 3.1257209004550384,\n",
       " 2.0043314847469156,\n",
       " 3.1675486467109795,\n",
       " 3.097381904668198,\n",
       " 3.1257209004550384,\n",
       " 3.117450721733805,\n",
       " 3.354774712217572,\n",
       " 3.3264357164307317,\n",
       " 3.3264357164307317,\n",
       " 2.3385786985645307,\n",
       " 3.1498773213133866,\n",
       " 3.1498773213133866,\n",
       " 3.1498773213133866,\n",
       " 3.1498773213133866,\n",
       " 2.7109772242416414,\n",
       " 2.7109772242416414,\n",
       " 2.7109772242416414,\n",
       " 2.778460946504958,\n",
       " 2.8603382491942146,\n",
       " 2.8775594241467366,\n",
       " 2.7723660220916555,\n",
       " 2.7811849928477717,\n",
       " 2.8863783949028528,\n",
       " 2.7811849928477717,\n",
       " 2.7935910925797796,\n",
       " 2.799686016993082,\n",
       " 2.799686016993082,\n",
       " 2.799686016993082,\n",
       " 2.6270088926746844,\n",
       " 2.6270088926746844,\n",
       " 2.249177562050571,\n",
       " 2.688077659122316,\n",
       " 2.688077659122316,\n",
       " 2.4859440326647118,\n",
       " 2.4859440326647118,\n",
       " 2.4859440326647118,\n",
       " 2.4859440326647118,\n",
       " 2.4859440326647118,\n",
       " 2.4859440326647118,\n",
       " 2.4859440326647118,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.4771014988950766,\n",
       " 2.442074838882777,\n",
       " 2.442074838882777,\n",
       " 2.442074838882777,\n",
       " 2.442074838882777,\n",
       " 2.442074838882777,\n",
       " 2.442074838882777,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.0031747418110317,\n",
       " 2.442074838882777,\n",
       " 2.463299909370901,\n",
       " 2.463299909370901,\n",
       " 2.463299909370901,\n",
       " 4.284633605985185,\n",
       " 1.987162327425735,\n",
       " 1.987162327425735,\n",
       " 5.120723053180002,\n",
       " 5.120723053180002,\n",
       " 5.137979826778313,\n",
       " 5.137979826778313,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.137979826778313,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.2331544289780245,\n",
       " 5.137979826778313,\n",
       " 5.137979826778313,\n",
       " 2.7996289351093937,\n",
       " 2.584250941469722,\n",
       " 2.7728774167362147,\n",
       " 2.7728774167362147,\n",
       " 2.584250941469722,\n",
       " 2.584250941469722,\n",
       " 2.7744501115155575,\n",
       " 2.7744501115155575,\n",
       " 2.7744501115155575,\n",
       " 2.869624713715272,\n",
       " 2.869624713715272,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 2.869624713715272,\n",
       " 2.869624713715272,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.207975605384188,\n",
       " 5.250323586299206,\n",
       " 5.250323586299206,\n",
       " 5.208058952543961,\n",
       " 5.208058952543961,\n",
       " 2.888260542773415,\n",
       " 2.582467722820736,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.6057838221865604,\n",
       " 2.778460946504958,\n",
       " 2.778460946504958,\n",
       " 2.778460946504958,\n",
       " 2.7811849928477717,\n",
       " 2.7811849928477717,\n",
       " 2.7811849928477717]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
