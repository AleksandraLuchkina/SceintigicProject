{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    def __init__(self):\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.split_gain = None\n",
    "        self.internal_value = None\n",
    "        self.node_index = None\n",
    "        self.leaf_value = None\n",
    "        self.tree_left = None\n",
    "        self.tree_right = None\n",
    "\n",
    "    def calc_predict_value(self, dataset):\n",
    "        if self.leaf_value is not None:\n",
    "            return self.leaf_value\n",
    "        elif dataset[self.split_feature] <= self.split_value:\n",
    "            return self.tree_left.calc_predict_value(dataset)\n",
    "        else:\n",
    "            return self.tree_right.calc_predict_value(dataset)\n",
    "\n",
    "    # print tree structure by JSON format\n",
    "    def describe_tree(self):\n",
    "        if not self.tree_left and not self.tree_right:\n",
    "            leaf_info = \"{leaf_value:\" + str(self.leaf_value) + \"}\"\n",
    "            return leaf_info\n",
    "        left_info = self.tree_left.describe_tree()\n",
    "        right_info = self.tree_right.describe_tree()\n",
    "        tree_structure = \"{split_feature:\" + str(self.split_feature) + \\\n",
    "                         \",split_value:\" + str(self.split_value) + \\\n",
    "                         \",split_gain:\" + str(self.split_gain) + \\\n",
    "                         \",internal_value:\" + str(self.internal_value) + \\\n",
    "                         \",node_index:\" + str(self.node_index) + \\\n",
    "                         \",left_tree:\" + left_info + \\\n",
    "                         \",right_tree:\" + right_info + \"}\"\n",
    "        return tree_structure\n",
    "\n",
    "    # count all leaf nodes & parent nodes which have two leaf nodes\n",
    "    def state_tree(self, leaves_state, node_state):\n",
    "        if not self.tree_left and not self.tree_right:\n",
    "            leaves_state.append(1)\n",
    "            return\n",
    "        if not self.tree_left.split_gain and not self.tree_right.split_gain:\n",
    "            node_state.append([self.node_index, self.split_gain])\n",
    "        self.tree_left.state_tree(leaves_state, node_state)\n",
    "        self.tree_right.state_tree(leaves_state, node_state)\n",
    "        return leaves_state, node_state\n",
    "\n",
    "    # prune tree with given node_index\n",
    "    def prune_tree(self, prune_node_index):\n",
    "        if not self.tree_left and not self.tree_right:\n",
    "            return\n",
    "        if self.tree_left.node_index == prune_node_index:\n",
    "            leaf_value = self.tree_left.internal_value\n",
    "            self.tree_left = Tree()\n",
    "            self.tree_left.node_index = prune_node_index\n",
    "            self.tree_left.leaf_value = leaf_value\n",
    "            return\n",
    "        elif self.tree_right.node_index == prune_node_index:\n",
    "            leaf_value = self.tree_right.internal_value\n",
    "            self.tree_right = Tree()\n",
    "            self.tree_right.node_index = prune_node_index\n",
    "            self.tree_right.leaf_value = leaf_value\n",
    "            return\n",
    "        self.tree_left.prune_tree(prune_node_index)\n",
    "        self.tree_right.prune_tree(prune_node_index)\n",
    "        return\n",
    "\n",
    "\n",
    "class BaseDecisionTree(object):\n",
    "    def __init__(self, max_depth, num_leaves, min_samples_split, min_samples_leaf, subsample,\n",
    "                 colsample_bytree, max_bin, min_child_weight, reg_gamma, reg_lambda, random_state):\n",
    "        self.max_depth = max_depth\n",
    "        self.num_leaves = num_leaves\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.max_bin = max_bin\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_gamma = reg_gamma\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.random_state = random_state\n",
    "        self.tree = Tree()\n",
    "        self.pred = None\n",
    "        self.node_index = 0\n",
    "        self.feature_importances_ = dict()\n",
    "\n",
    "    def fit(self, dataset, targets):\n",
    "        dataset_copy = copy.deepcopy(dataset).reset_index(drop=True)\n",
    "        targets_copy = copy.deepcopy(targets).reset_index(drop=True)\n",
    "\n",
    "        if self.random_state:\n",
    "            random.seed(self.random_state)\n",
    "        if self.subsample < 1.0:\n",
    "            subset_index = random.sample(range(len(targets)), int(self.subsample*len(targets)))\n",
    "            dataset_copy = dataset_copy.iloc[subset_index, :].reset_index(drop=True)\n",
    "            targets_copy = targets_copy.iloc[subset_index, :].reset_index(drop=True)\n",
    "        if self.colsample_bytree < 1.0:\n",
    "            subcol_index = random.sample(list(dataset_copy.columns), int(self.colsample_bytree*len(dataset_copy.columns)))\n",
    "            dataset_copy = dataset_copy[subcol_index]\n",
    "\n",
    "        self.tree = self._fit(dataset_copy, targets_copy, depth=0)\n",
    "        self.pred = dataset.apply(lambda x: self.predict(x), axis=1)\n",
    "\n",
    "        leaves_state, node_state = self.tree.state_tree(leaves_state=[], node_state=[])\n",
    "        while sum(leaves_state) > self.num_leaves:\n",
    "            node_state = sorted(node_state, key=lambda x: x[1])\n",
    "            self.tree.prune_tree(node_state[0][0])\n",
    "            leaves_state, node_state = self.tree.state_tree(leaves_state=[], node_state=[])\n",
    "        return self\n",
    "\n",
    "    def _fit(self, dataset, targets, depth):\n",
    "        if dataset.__len__() <= self.min_samples_split or targets['hess'].sum() <= self.min_child_weight:\n",
    "            tree = Tree()\n",
    "            tree.leaf_value = self.calc_leaf_value(targets)\n",
    "            return tree\n",
    "\n",
    "        if depth < self.max_depth:\n",
    "            best_split_feature, best_split_value, best_split_gain, best_internal_value = \\\n",
    "                self.choose_best_feature(dataset, targets)\n",
    "            left_dataset, right_dataset, left_targets, right_targets = \\\n",
    "                self.split_dataset(dataset, targets, best_split_feature, best_split_value)\n",
    "\n",
    "            tree = Tree()\n",
    "            if left_dataset.__len__() <= self.min_samples_leaf or \\\n",
    "                    right_dataset.__len__() <= self.min_samples_leaf:\n",
    "                tree.leaf_value = self.calc_leaf_value(targets)\n",
    "                return tree\n",
    "            else:\n",
    "                self.feature_importances_[best_split_feature] = \\\n",
    "                    self.feature_importances_.get(best_split_feature, 0) + 1\n",
    "\n",
    "                tree.split_feature = best_split_feature\n",
    "                tree.split_value = best_split_value\n",
    "                tree.split_gain = best_split_gain\n",
    "                tree.internal_value = best_internal_value\n",
    "                tree.node_index = self.node_index\n",
    "                self.node_index += 1\n",
    "                tree.tree_left = self._fit(left_dataset, left_targets, depth+1)\n",
    "                tree.tree_right = self._fit(right_dataset, right_targets, depth+1)\n",
    "                return tree\n",
    "        else:\n",
    "            tree = Tree()\n",
    "            tree.leaf_value = self.calc_leaf_value(targets)\n",
    "            return tree\n",
    "\n",
    "    def choose_best_feature(self, dataset, targets):\n",
    "        best_split_gain = float('-inf')\n",
    "        best_split_feature = None\n",
    "        best_split_value = None\n",
    "\n",
    "        for feature in dataset.columns:\n",
    "            if dataset[feature].unique().__len__() <= 100:\n",
    "                unique_values = dataset[feature].unique()\n",
    "            else:\n",
    "                unique_values = np.unique([np.percentile(dataset[feature], x)\n",
    "                                           for x in np.linspace(0, 100, self.max_bin)])\n",
    "\n",
    "            for split_value in unique_values:\n",
    "                left_targets = targets[dataset[feature] <= split_value]\n",
    "                right_targets = targets[dataset[feature] > split_value]\n",
    "                split_gain = self.calc_split_gain(left_targets, right_targets)\n",
    "\n",
    "                if split_gain > best_split_gain:\n",
    "                    best_split_feature = feature\n",
    "                    best_split_value = split_value\n",
    "                    best_split_gain = split_gain\n",
    "        best_internal_value = self.calc_leaf_value(targets)\n",
    "        return best_split_feature, best_split_value, best_split_gain, best_internal_value\n",
    "\n",
    "    def calc_leaf_value(self, targets):\n",
    "        leaf_value = - targets['grad'].sum() / (targets['hess'].sum() + self.reg_lambda)\n",
    "        return leaf_value\n",
    "\n",
    "    def calc_split_gain(self, left_targets, right_targets):\n",
    "        left_grad = left_targets['grad'].sum()\n",
    "        left_hess = left_targets['hess'].sum()\n",
    "        right_grad = right_targets['grad'].sum()\n",
    "        right_hess = right_targets['hess'].sum()\n",
    "        split_gain = 0.5 * (left_grad ** 2 / (left_hess + self.reg_lambda) +\n",
    "                            right_grad ** 2 / (right_hess + self.reg_lambda) -\n",
    "                            (left_grad + right_grad) ** 2 / (left_hess + right_hess + self.reg_lambda)) - self.reg_gamma\n",
    "        return split_gain\n",
    "\n",
    "    @staticmethod\n",
    "    def split_dataset(dataset, targets, split_feature, split_value):\n",
    "        left_dataset = dataset[dataset[split_feature] <= split_value]\n",
    "        left_targets = targets[dataset[split_feature] <= split_value]\n",
    "        right_dataset = dataset[dataset[split_feature] > split_value]\n",
    "        right_targets = targets[dataset[split_feature] > split_value]\n",
    "        return left_dataset, right_dataset, left_targets, right_targets\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        return self.tree.calc_predict_value(dataset)\n",
    "\n",
    "    def print_tree(self):\n",
    "        return self.tree.describe_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================iter: 1=====================================\n",
      "====================================iter: 2=====================================\n",
      "====================================iter: 3=====================================\n",
      "====================================iter: 4=====================================\n",
      "====================================iter: 5=====================================\n",
      "[[0.02528146 0.97471854]\n",
      " [0.02427736 0.97572264]\n",
      " [0.02522805 0.97477195]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02842016 0.97157984]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.03522683 0.96477317]\n",
      " [0.03423381 0.96576619]\n",
      " [0.03423381 0.96576619]\n",
      " [0.03186597 0.96813403]\n",
      " [0.02514387 0.97485613]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02848012 0.97151988]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02528146 0.97471854]\n",
      " [0.02528146 0.97471854]\n",
      " [0.02528146 0.97471854]\n",
      " [0.02528146 0.97471854]\n",
      " [0.02657948 0.97342052]\n",
      " [0.03296027 0.96703973]\n",
      " [0.02994294 0.97005706]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.02427736 0.97572264]\n",
      " [0.02422602 0.97577398]\n",
      " [0.02422602 0.97577398]\n",
      " [0.02291453 0.97708547]\n",
      " [0.02291453 0.97708547]\n",
      " [0.02514387 0.97485613]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02285868 0.97714132]\n",
      " [0.02285868 0.97714132]\n",
      " [0.02285868 0.97714132]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02285868 0.97714132]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00440254 0.99559746]\n",
      " [0.02422602 0.97577398]\n",
      " [0.02657948 0.97342052]\n",
      " [0.03423381 0.96576619]\n",
      " [0.02628496 0.97371504]\n",
      " [0.02386364 0.97613636]\n",
      " [0.02514387 0.97485613]\n",
      " [0.02514387 0.97485613]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02285868 0.97714132]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02285868 0.97714132]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02522805 0.97477195]\n",
      " [0.02522805 0.97477195]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02848012 0.97151988]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02848012 0.97151988]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02848012 0.97151988]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02657948 0.97342052]\n",
      " [0.03598584 0.96401416]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02514387 0.97485613]\n",
      " [0.03186597 0.96813403]\n",
      " [0.03186597 0.96813403]\n",
      " [0.03186597 0.96813403]\n",
      " [0.03396535 0.96603465]\n",
      " [0.03396535 0.96603465]\n",
      " [0.02952414 0.97047586]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02842016 0.97157984]\n",
      " [0.02842016 0.97157984]\n",
      " [0.02842016 0.97157984]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02842016 0.97157984]\n",
      " [0.02952414 0.97047586]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02958636 0.97041364]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02663567 0.97336433]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02657948 0.97342052]\n",
      " [0.02842016 0.97157984]\n",
      " [0.02952414 0.97047586]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00450049 0.99549951]\n",
      " [0.00440254 0.99559746]\n",
      " [0.00441205 0.99558795]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.0033888  0.9966112 ]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00441205 0.99558795]\n",
      " [0.00440254 0.99559746]]\n",
      "[1 1 1 2 2 1 2 1 1 1 1 3 3 1 3 3 3 2 2 2 2 2 2 3 3 3 3 3 3 2 2 2 2 2 2 2 2\n",
      " 2 3 3 3 3 3 3 3 6 8 8 8 8 8 8 8 8 8 6 6 6 6 5 5 5 5 5 5 6 6 6 6 5 5 5 5 8\n",
      " 8 8 8 8 8 8 8 6 6 6 6 6 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 5 5 6 6 8 8 8 8 8 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8 5 5 5 6 6 6 6 6 6 6 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 3 3 3 3 3 3 3 3 3 3 3 2 2 2 5 5 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 5 5 5 6 6 5 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 2 2 8 8 8 7 7 7 7 7 7\n",
      " 7 7 7 7 7 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 5 8 8 8 8 8 5 5 5 5 5 3 3 3\n",
      " 3 3 3 3 3 3 8 8 8 8 8 8 7 7 7 8 8 8 8 8 8 8 8 7 7 7 7 7 6 6 6 7 7 7 7 7 7\n",
      " 7 7 5 5 5 5 5 5 5 5 5 5 5 6 6 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2\n",
      " 2 3 3 2 2 2 2 2 2 2 2 3 3 5 5 5 5 5 5 8 8 8 8 8 6 6 6 6 5 5 5 6 6 6 8 5 5\n",
      " 6 5 6 6 6 6 6 6 5 6 6 6 9 9 9 9 9 9 9 6 4 4 4 4 4 4 4 4 6 6 6 4]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4386f2c25f0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import exp, log\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('precision', 4)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "\n",
    "\n",
    "class BaseLoss(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def grad(self, targets):\n",
    "        pass\n",
    "\n",
    "    def hess(self, targets):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SquareLoss(BaseLoss):\n",
    "    \"\"\"\n",
    "    L = 0.5*(pred - label)**2\n",
    "    \"\"\"\n",
    "    def grad(self, targets):\n",
    "        grad = targets['pred'] - targets['label']\n",
    "        return grad\n",
    "\n",
    "    def hess(self, targets):\n",
    "        hess = 1\n",
    "        return hess\n",
    "\n",
    "\n",
    "class LogisticLoss(BaseLoss):\n",
    "    \"\"\"\n",
    "    L = log(1 + exp(-label*pred))\n",
    "    \"\"\"\n",
    "    def grad(self, targets):\n",
    "        pred = 1.0 / (1.0 + exp(- targets['pred']))\n",
    "        grad = - targets['label'] / (1 + exp(targets['label'] * pred))\n",
    "        return grad\n",
    "\n",
    "    def hess(self, targets):\n",
    "        pred = 1.0 / (1.0 + exp(- targets['pred']))\n",
    "        hess = exp(targets['label'] * pred) / (1 + exp(targets['label'] * pred))**2\n",
    "        return hess\n",
    "\n",
    "\n",
    "class XGBClassifier(object):\n",
    "    def __init__(self, n_estimators=100, max_depth=-1, num_leaves=-1, learning_rate=0.1, min_samples_split=2,\n",
    "                 min_samples_leaf=1, subsample=1., colsample_bytree=1., max_bin=225, min_child_weight=1.,\n",
    "                 reg_gamma=0., reg_lambda=0., loss=\"squareloss\", random_state=None):\n",
    "        \"\"\"Construct a xgboost model\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_estimators : int, optional (default=100)\n",
    "            Number of boosted trees to fit.\n",
    "        max_depth : int, optional (default=-1)\n",
    "            Maximum tree depth for base learners, -1 means no limit.\n",
    "        num_leaves : int, optional (default=-1)\n",
    "            Maximum tree leaves for base learners, -1 means no limit.\n",
    "        learning_rate : float, optional (default=0.1)\n",
    "            Boosting learning rate.\n",
    "        min_samples_split : int, optional (default=2)\n",
    "            The minimum number of samples required to split an internal node.\n",
    "        min_samples_leaf : int, optional (default=1)\n",
    "            The minimum number of samples required to be at a leaf node.\n",
    "        subsample : float, optional (default=1.)\n",
    "            Subsample ratio of the training instance.\n",
    "        colsample_bytree : float, optional (default=1.)\n",
    "            Subsample ratio of columns when constructing each tree.\n",
    "        max_bin: int or None, optional (default=225))\n",
    "            Max number of discrete bins for features.\n",
    "        min_child_weight : float, optional (default=1.)\n",
    "            Minimum sum of instance weight(hessian) needed in a child(leaf).\n",
    "        reg_gamma : float, optional (default=0.)\n",
    "            L1 regularization term on weights.\n",
    "        reg_lambda : float, optional (default=0.)\n",
    "            L2 regularization term on weights.\n",
    "        loss: loss object, (default=\"logistic\")\n",
    "            logisticloss, squareloss\n",
    "        random_state : int or None, optional (default=None)\n",
    "            Random number seed.\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth if max_depth != -1 else float('inf')\n",
    "        self.num_leaves = num_leaves if num_leaves != -1 else float('inf')\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.max_bin = max_bin\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_gamma = reg_gamma\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.loss = loss\n",
    "        self.random_state = random_state\n",
    "        self.pred_0 = None\n",
    "        self.trees = dict()\n",
    "        self.feature_importances_ = dict()\n",
    "\n",
    "    def fit(self, dataset, targets):\n",
    "        if self.loss == \"logistic\":\n",
    "            self.loss = LogisticLoss()\n",
    "        elif self.loss == \"squareloss\":\n",
    "            self.loss = SquareLoss()\n",
    "        else:\n",
    "            raise ValueError(\"The loss function must be 'logistic' or 'squareloss'!\")\n",
    "\n",
    "        targets = targets.to_frame(name='label')\n",
    "#         if targets['label'].unique().__len__() != 2:\n",
    "#             raise ValueError(\"There must be two class for targets!\")\n",
    "#         if len([x for x in dataset.columns if dataset[x].dtype in ['int32', 'float32', 'int64', 'float64']]) \\\n",
    "#                 != len(dataset.columns):\n",
    "#             raise ValueError(\"The features dtype must be int or float!\")\n",
    "\n",
    "        if self.random_state:\n",
    "            random.seed(self.random_state)\n",
    "        random_state_stages = random.sample(range(max(self.n_estimators, len(targets))), self.n_estimators)\n",
    "\n",
    "        # the first base function\n",
    "        mean = 1.0 * sum(targets['label']) / len(targets['label'])\n",
    "#         self.pred_0 = 0.5 * log((1 + mean) / (1 - mean))\n",
    "        self.pred_0 = 0.5\n",
    "        targets['pred'] = self.pred_0\n",
    "        targets['grad'] = targets.apply(self.loss.grad, axis=1)\n",
    "        targets['hess'] = targets.apply(self.loss.hess, axis=1)\n",
    "\n",
    "        for stage in range(self.n_estimators):\n",
    "            print((\"iter: \"+str(stage+1)).center(80, '='))\n",
    "            tree = BaseDecisionTree(self.max_depth, self.num_leaves, self.min_samples_split, self.min_samples_leaf,\n",
    "                                    self.subsample, self.colsample_bytree, self.max_bin, self.min_child_weight,\n",
    "                                    self.reg_gamma, self.reg_lambda, random_state_stages[stage])\n",
    "            tree.fit(dataset, targets)\n",
    "            self.trees[stage] = tree\n",
    "            targets['pred'] = targets['pred'] + self.learning_rate * tree.pred\n",
    "            targets['grad'] = targets.apply(self.loss.grad, axis=1)\n",
    "            targets['hess'] = targets.apply(self.loss.hess, axis=1)\n",
    "\n",
    "            for key, value in tree.feature_importances_.items():\n",
    "                self.feature_importances_[key] = self.feature_importances_.get(key, 0) + 1\n",
    "\n",
    "    def predict_proba(self, dataset):\n",
    "        res = []\n",
    "        for index, row in dataset.iterrows():\n",
    "            f_value = self.pred_0\n",
    "            for stage, tree in self.trees.items():\n",
    "                f_value += self.learning_rate * tree.predict(row)\n",
    "            p_0 = 1.0 / (1 + exp(2 * f_value))\n",
    "            res.append([p_0, 1 - p_0])\n",
    "        return np.array(res)\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        res = []\n",
    "        for p in self.predict_proba(dataset):\n",
    "            label = 0 if p[0] >= p[1] else 1\n",
    "            res.append(label)\n",
    "        return np.array(res)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     df = pd.read_csv(\"test.csv\")\n",
    "    df = pd.read_csv('facies_vectors.csv')\n",
    "    df = df.fillna(df['PE'].mean())\n",
    "    feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "    well = 'KIMZEY A'\n",
    "    test = df[df['Well Name'] == well]\n",
    "    train = df[df['Well Name'] != well]\n",
    "    X_train = train[feature_names].values \n",
    "    y_train = train['Facies'].values \n",
    "    X_test = test[feature_names].values \n",
    "    y_test = test['Facies'].values \n",
    "\n",
    "    xgb = XGBClassifier(n_estimators=5,\n",
    "                        max_depth=6,\n",
    "                        num_leaves=30,\n",
    "                        learning_rate=0.1,\n",
    "                        min_samples_split=40,\n",
    "                        min_samples_leaf=10,\n",
    "                        subsample=0.6,\n",
    "                        colsample_bytree=0.8,\n",
    "                        max_bin=150,\n",
    "                        min_child_weight=1,\n",
    "                        reg_gamma=0.1,\n",
    "                        reg_lambda=0.3,\n",
    "                        loss='logistic',\n",
    "                        random_state=66)\n",
    "    train_count = int(0.7 * len(df))\n",
    "    xgb.fit(train[feature_names], train['Facies'])\n",
    "#     xgb.fit(df.ix[:train_count, :-1], df.ix[:train_count, 'Class'])\n",
    "\n",
    "    y_res = xgb.predict_proba(test[feature_names])\n",
    "    print(y_res)\n",
    "    print(y_test)\n",
    "    acc = 0\n",
    "    for index in range (len(y_res)):\n",
    "        if (y_res[index] == y_test[index]):\n",
    "            acc += 1\n",
    "    print(\"Accuracy is \", acc/len(y_res))       \n",
    "\n",
    "#     from sklearn import metrics\n",
    "#     print (metrics.roc_auc_score(df.ix[:train_count, 'Class'], xgb.predict_proba(df.ix[:train_count, :-1])[:, 1]))\n",
    "#     print (metrics.roc_auc_score(df.ix[train_count:, 'Class'], xgb.predict_proba(df.ix[train_count:, :-1])[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
