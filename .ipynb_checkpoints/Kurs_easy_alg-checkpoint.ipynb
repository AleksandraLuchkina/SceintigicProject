{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('facies_vectors.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes_algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, 7:3, acc_gaus =  0.18048192771084337\n",
      "mean, 7:3, acc_bern =  0.49670682730923693\n",
      "mean, well, acc_gaus =  0.2372882516172778\n",
      "mean, well, acc_bern =  0.3876463620579104\n",
      "without PE, 7:3, acc_gaus =  0.16313253012048193\n",
      "without PE, 7:3, acc_bern =  0.48313253012048196\n",
      "without PE, well, acc_gaus =  0.21930275634616345\n",
      "without PE, well, acc_bern =  0.38067969134840074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# with mean value\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "data = data.fillna(data['PE'].mean())\n",
    "## with split\n",
    "acc_gaus = 0\n",
    "acc_bern = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_gaus = GaussianNB()\n",
    "    clf_gaus.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_gaus.predict(X_test_norm)\n",
    "    acc_gaus += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "    \n",
    "    clf_bern = BernoulliNB(alpha=1, binarize=0.0)\n",
    "    clf_bern.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_bern.predict(X_test_norm)\n",
    "    acc_bern += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('mean, 7:3, acc_gaus = ', acc_gaus/10)\n",
    "print('mean, 7:3, acc_bern = ', acc_bern/10)\n",
    "\n",
    "## with test well\n",
    "params_bern= dict()\n",
    "params_bern['LUKE G U'] = {'alpha': 10, 'binarize': -1.0}\n",
    "params_bern['KIMZEY A'] = {'alpha': 10, 'binarize': -1.0}\n",
    "params_bern['CROSS H CATTLE'] = {'alpha': 5, 'binarize': 0.0}\n",
    "params_bern['NEWBY'] = {'alpha': 10, 'binarize': -1.0}\n",
    "params_bern['SHRIMPLIN'] = {'alpha': 1, 'binarize': 0.0}\n",
    "params_bern['ALEXANDER D'] = {'alpha': 10, 'binarize': -1.0}\n",
    "params_bern['SHANKLE'] = {'alpha': 10, 'binarize': 0.0}\n",
    "params_bern['CHURCHMAN BIBLE'] ={'alpha': 1, 'binarize': 1.0}\n",
    "params_bern['Recruit F9'] = {'alpha': 5, 'binarize': -1.0}\n",
    "params_bern['NOLAN'] = {'alpha': 1, 'binarize': -1.0}\n",
    "acc_gaus = 0\n",
    "acc_bern = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_gaus = GaussianNB()\n",
    "    clf_gaus.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_gaus.predict(X_test_norm)\n",
    "    acc_gaus += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "    \n",
    "    clf_bern = BernoulliNB(alpha=params_bern[well]['alpha'], binarize=params_bern[well]['binarize'])\n",
    "    clf_bern.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_bern.predict(X_test_norm)\n",
    "    acc_bern += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('mean, well, acc_gaus = ', acc_gaus/10)\n",
    "print('mean, well, acc_bern = ', acc_bern/10)\n",
    "\n",
    "# without PE\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']\n",
    "## with split\n",
    "acc_gaus = 0\n",
    "acc_bern = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_gaus = GaussianNB()\n",
    "    clf_gaus.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_gaus.predict(X_test_norm)\n",
    "    acc_gaus += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "    \n",
    "    clf_bern = BernoulliNB(alpha=1, binarize=0.0)\n",
    "    clf_bern.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_bern.predict(X_test_norm)\n",
    "    acc_bern += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, 7:3, acc_gaus = ', acc_gaus/10)\n",
    "print('without PE, 7:3, acc_bern = ', acc_bern/10)\n",
    "\n",
    "## with test well\n",
    "acc_gaus = 0\n",
    "acc_bern = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_gaus = GaussianNB()\n",
    "    clf_gaus.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_gaus.predict(X_test_norm)\n",
    "    acc_gaus += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "    \n",
    "    clf_bern = BernoulliNB(alpha=params_bern[well]['alpha'], binarize=params_bern[well]['binarize'])\n",
    "    clf_bern.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_bern.predict(X_test_norm)\n",
    "    acc_bern += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, well, acc_gaus = ', acc_gaus/10)\n",
    "print('without PE, well, acc_bern = ', acc_bern/10)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, 7:3, acc_svm =  0.6652208835341364\n",
      "mean, well, acc_svm =  0.5033048652200915\n",
      "without PE, 7:3, acc_svm =  0.6326104417670682\n",
      "without PE, well, acc_svm =  0.4973203528611485\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# with mean value\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "data = data.fillna(data['PE'].mean())\n",
    "## with split\n",
    "acc_svm = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_svm = SVC(C=10, degree=2, kernel='rbf')\n",
    "    clf_svm.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_svm.predict(X_test_norm)\n",
    "    acc_svm += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('mean, 7:3, acc_svm = ', acc_svm/10)\n",
    "\n",
    "## with test well\n",
    "params_svm = dict()\n",
    "params_svm['LUKE G U'] = {'C': 1, 'degree': 3, 'kernel': 'poly'}\n",
    "params_svm['KIMZEY A'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['CROSS H CATTLE'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['NEWBY'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['SHRIMPLIN'] = {'C': 10, 'degree': 2, 'kernel': 'linear'}\n",
    "params_svm['ALEXANDER D'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['SHANKLE'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['CHURCHMAN BIBLE'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['Recruit F9'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "params_svm['NOLAN'] = {'C': 1, 'degree': 2, 'kernel': 'rbf'}\n",
    "acc_svm = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_svm = SVC(C=params_svm[well]['C'], degree=params_svm[well]['degree'], kernel=params_svm[well]['kernel'])\n",
    "    clf_svm.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_svm.predict(X_test_norm)\n",
    "    acc_svm += f1_score(y_test, y_predict, average='micro')\n",
    "        \n",
    "print('mean, well, acc_svm = ', acc_svm/10)\n",
    "\n",
    "# without PE\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']\n",
    "## with split\n",
    "acc_svm = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_svm = SVC(C=10, degree=2, kernel='rbf')\n",
    "    clf_svm.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_svm.predict(X_test_norm)\n",
    "    acc_svm += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, 7:3, acc_svm = ', acc_svm/10)\n",
    "\n",
    "## with test well\n",
    "acc_svm = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_svm = SVC(C=params_svm[well]['C'], degree=params_svm[well]['degree'], kernel=params_svm[well]['kernel'])\n",
    "    clf_svm.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_svm.predict(X_test_norm)\n",
    "    acc_svm += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, well, acc_svm = ', acc_svm/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, 7:3, acc_lr =  0.5606425702811246\n",
      "mean, well, acc_lr =  0.4523152952415469\n",
      "without PE, 7:3, acc_lr =  0.5329317269076305\n",
      "without PE, well, acc_lr =  0.4339687309424393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# with mean value\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "data = data.fillna(data['PE'].mean())\n",
    "## with split\n",
    "acc_lr = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_lr = LogisticRegression(C=10, class_weight=None, fit_intercept=True, multi_class='ovr', solver='saga')\n",
    "    clf_lr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_lr.predict(X_test_norm)\n",
    "    acc_lr += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('mean, 7:3, acc_lr = ', acc_lr/10)\n",
    "\n",
    "## with test well\n",
    "params_logr = dict()\n",
    "params_logr['LUKE G U'] = {'C': 1, 'class_weight': None, 'fit_intercept': True, 'multi_class': 'ovr', 'solver': 'saga'}\n",
    "params_logr['KIMZEY A'] = {'C': 1, 'class_weight': None, 'fit_intercept': True, 'multi_class': 'ovr', 'solver': 'lbfgs'}\n",
    "params_logr['CROSS H CATTLE'] = {'C': 1, 'class_weight': None, 'fit_intercept': False, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
    "params_logr['NEWBY'] = {'C': 10, 'class_weight': None, 'fit_intercept': True, 'multi_class': 'ovr', 'solver': 'sag'}\n",
    "params_logr['SHRIMPLIN'] = {'C': 5, 'class_weight': None, 'fit_intercept': False, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
    "params_logr['ALEXANDER D'] = {'C': 5, 'class_weight': None, 'fit_intercept': False, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
    "params_logr['SHANKLE'] = {'C': 1, 'class_weight': None, 'fit_intercept': True, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
    "params_logr['CHURCHMAN BIBLE'] = {'C': 1, 'class_weight': None, 'fit_intercept': False, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
    "params_logr['Recruit F9'] = {'C': 10, 'class_weight': None, 'fit_intercept': True, 'multi_class': 'ovr', 'solver': 'sag'}\n",
    "params_logr['NOLAN'] = {'C': 1, 'class_weight': None, 'fit_intercept': True, 'multi_class': 'ovr', 'solver': 'newton-cg'}\n",
    "acc_lr = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_lr = LogisticRegression(C=params_logr[well]['C'], class_weight=params_logr[well]['class_weight'],\n",
    "                                fit_intercept=params_logr[well]['fit_intercept'], multi_class=params_logr[well]['multi_class'],\n",
    "                                solver=params_logr[well]['solver'])\n",
    "    clf_lr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_lr.predict(X_test_norm)\n",
    "    acc_lr += f1_score(y_test, y_predict, average='micro')\n",
    "        \n",
    "print('mean, well, acc_lr = ', acc_lr/10)\n",
    "\n",
    "# without PE\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']\n",
    "## with split\n",
    "acc_lr = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_lr = LogisticRegression(C=10, class_weight=None, fit_intercept=True, multi_class='ovr', solver='saga')\n",
    "    clf_lr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_lr.predict(X_test_norm)\n",
    "    acc_lr += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, 7:3, acc_lr = ', acc_lr/10)\n",
    "\n",
    "## with test well\n",
    "acc_lr = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_lr = LogisticRegression(C=params_logr[well]['C'], class_weight=params_logr[well]['class_weight'],\n",
    "                                fit_intercept=params_logr[well]['fit_intercept'], multi_class=params_logr[well]['multi_class'],\n",
    "                                solver=params_logr[well]['solver'])\n",
    "    clf_lr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_lr.predict(X_test_norm)\n",
    "    acc_lr += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, well, acc_lr = ', acc_lr/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean, 7:3, acc_dtr =  0.6132530120481927\n",
      "mean, well, acc_dtr =  0.49514352865282885\n",
      "without PE, 7:3, acc_dtr =  0.5963855421686747\n",
      "without PE, well, acc_dtr =  0.4786621983193104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# with mean value\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "data = data.fillna(data['PE'].mean())\n",
    "## with split\n",
    "acc_dtr = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_dtr = DecisionTreeClassifier(criterion='entropy', max_depth= None, max_features= None, min_samples_split= 3, splitter= 'best')\n",
    "    clf_dtr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_dtr.predict(X_test_norm)\n",
    "    acc_dtr += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('mean, 7:3, acc_dtr = ', acc_dtr/10)\n",
    "\n",
    "## with test well\n",
    "params = dict()\n",
    "params['LUKE G U'] = {'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'} \n",
    "params['KIMZEY A'] = {'criterion': 'entropy', 'max_depth': 4, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "params['CROSS H CATTLE'] = {'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "params['NEWBY'] = {'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'min_samples_split': 4, 'splitter': 'best'}\n",
    "params['SHRIMPLIN'] = {'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "params['ALEXANDER D'] = {'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "params['SHANKLE'] = {'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'min_samples_split': 4, 'splitter': 'best'}\n",
    "params['CHURCHMAN BIBLE'] = {'criterion': 'gini', 'max_depth': 3, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "params['Recruit F9'] = {'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "params['NOLAN'] = {'criterion': 'gini', 'max_depth': 5, 'max_features': None, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "acc_dtr = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_dtr = DecisionTreeClassifier(criterion=params[well]['criterion'], max_depth=params[well]['max_depth'], max_features = params[well]['max_features'],\n",
    "                                     min_samples_split=params[well]['min_samples_split'], splitter=params[well]['splitter'])\n",
    "    clf_dtr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_dtr.predict(X_test_norm)\n",
    "    acc_dtr += f1_score(y_test, y_predict, average='micro')\n",
    "        \n",
    "print('mean, well, acc_dtr = ', acc_dtr/10)\n",
    "\n",
    "# without PE\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'NM_M', 'RELPOS']\n",
    "## with split\n",
    "acc_dtr = 0\n",
    "for index in range(10):\n",
    "    train, test = train_test_split(data, test_size=0.3)\n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    \n",
    "    clf_dtr = DecisionTreeClassifier(criterion='entropy', max_depth= None, max_features= None, min_samples_split= 3, splitter= 'best')\n",
    "    clf_dtr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_dtr.predict(X_test_norm)\n",
    "    acc_dtr += f1_score(y_test, y_predict, average='micro')\n",
    "print('without PE, 7:3, acc_dtr = ', acc_dtr/10)\n",
    "\n",
    "## with test well\n",
    "acc_dtr = 0\n",
    "for well in set(data['Well Name']):\n",
    "    train = data[data['Well Name'] != well]\n",
    "    test = data[data['Well Name'] == well]\n",
    "        \n",
    "    X_train = train[feature_names].values\n",
    "    y_train = train['Facies'].values\n",
    "    X_test = test[feature_names].values\n",
    "    y_test = test['Facies'].values\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_norm = scaler.transform(X_train)\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "    clf_dtr = DecisionTreeClassifier(criterion=params[well]['criterion'], max_depth=params[well]['max_depth'], max_features = params[well]['max_features'], min_samples_split=params[well]['min_samples_split'], splitter=params[well]['splitter'])\n",
    "    clf_dtr.fit(X_train_norm , y_train)\n",
    "    y_predict = clf_dtr.predict(X_test_norm)\n",
    "    acc_dtr += f1_score(y_test, y_predict, average='micro')\n",
    "    \n",
    "print('without PE, well, acc_dtr = ', acc_dtr/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
